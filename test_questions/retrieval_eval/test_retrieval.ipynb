{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# OS environ\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "os.environ[\"TAVILY_API_KEY\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from typing import List\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.runnables import chain\n",
    "\n",
    "\n",
    "vectorstore = Chroma(\n",
    "    persist_directory=\"./yahoo-finance-chroma\",\n",
    "    embedding_function=OpenAIEmbeddings(),\n",
    "    collection_name=\"yahoo-news\",\n",
    "    collection_metadata={\"hnsw:space\": \"cosine\"},\n",
    ")\n",
    "\n",
    "\n",
    "@chain\n",
    "def retriever(query: str) -> List[Document]:\n",
    "    docs, distances = zip(*vectorstore.similarity_search_with_relevance_scores(query))\n",
    "\n",
    "    for doc, distance in zip(docs, distances):\n",
    "        doc.metadata[\"score\"] = distance * 2\n",
    "\n",
    "    return docs\n",
    "\n",
    "\n",
    "\n",
    "def retrieve(state):\n",
    "    \"\"\"\n",
    "    Retrieve documents\n",
    "    Args:\n",
    "        state (dict): The current state of the agent, including all keys.\n",
    "    Returns:\n",
    "        dict: New key added to state, documents, that contains documents.\n",
    "    \"\"\"\n",
    "    print(\"---RETRIEVE---\")\n",
    "\n",
    "    state_dict = state[\"keys\"]\n",
    "    question = state_dict[\"question\"]\n",
    "    documents = retriever.invoke(question)\n",
    "\n",
    "    return {\"keys\": {\"documents\": documents, \"question\": question}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate retrieval performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "openai_client = OpenAI()\n",
    "\n",
    "def eval_qrel(query, docs):\n",
    "\n",
    "    \"\"\"\n",
    "    Use GPT to evaluate the relevance of the retrieved documents\n",
    "    \"\"\"\n",
    "\n",
    "    content = f\"\"\"\n",
    "        Evaluate the relevance of the following 4 documents to the query: {query}. Rank each document with an integer relevance score from 0 to 2, where 0 is not relevant and 2 is relevant.\n",
    "        \n",
    "        Here are the 4 documents in json format: \n",
    "        {[{\"title\": doc.metadata[\"title\"], \"content\": doc.page_content} for doc in docs]}\n",
    "        \n",
    "        Return the relevance scores for each document in an array, for example: [0, 1, 0, 2]. No need to include the query in the response or explain the relevance score.\n",
    "    \"\"\"\n",
    "\n",
    "    completion = openai_client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": content,\n",
    "            },\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    return completion.choices[0].message.content\n",
    "\n",
    "\n",
    "def get_qrel(query, retrieved_docs) -> List[int]:\n",
    "    \"\"\"\n",
    "    Get evaluation from GPT and return the relevance scores array\n",
    "    \"\"\"\n",
    "    qrel = eval_qrel(query, retrieved_docs)\n",
    "    qrel = qrel.replace(\"[\", \"\").replace(\"]\", \"\").split(\", \")\n",
    "    qrel = [int(i) for i in qrel]\n",
    "    qrel = qrel[: len(retrieved_docs)]\n",
    "    return qrel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ir_measures\n",
    "from ir_measures import Precision, MRR, nDCG\n",
    "\n",
    "def form_irmeasure_qrel_arr(query: str, retrieved_docs: List[Document], qrel: List[int]):\n",
    "    \"\"\"\n",
    "    Format the qrels array for later use in ir_measures\n",
    "    \"\"\"\n",
    "    \n",
    "    if len(qrel) < len(retrieved_docs):\n",
    "        qrel = qrel + [0] * (len(retrieved_docs) - len(qrel))\n",
    "\n",
    "    return [\n",
    "        ir_measures.Qrel(query, doc.page_content, qrel[i])\n",
    "        for i, doc in enumerate(retrieved_docs)\n",
    "    ]\n",
    "\n",
    "\n",
    "def form_irmeasure_run_arr(query: str, retrieved_docs: List[Document]):\n",
    "    \"\"\"\n",
    "    Format the runs array for later use in ir_measures\n",
    "    \"\"\"\n",
    "\n",
    "    return [\n",
    "        ir_measures.ScoredDoc(query, doc.page_content, doc.metadata[\"score\"])\n",
    "        for doc in retrieved_docs\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "final_quests = json.load(open(\"./questions/final_questions.json\"))[:100]\n",
    "final_quests = [quest[\"question\"] for quest in final_quests]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ir_qrels = []\n",
    "ir_runs = []\n",
    "\n",
    "for i, quest in enumerate(final_quests):\n",
    "    print(f\"Question {i+1}/{len(final_quests)}\")\n",
    "    \n",
    "    state = {\"keys\": {\"question\": quest}}\n",
    "    state = retrieve(state)\n",
    "    \n",
    "    retrieved_docs = state[\"keys\"][\"documents\"]\n",
    "    qrel = get_qrel(quest, retrieved_docs)\n",
    "    \n",
    "    ir_qrels.extend(form_irmeasure_qrel_arr(quest, retrieved_docs, qrel))\n",
    "    ir_runs.extend(form_irmeasure_run_arr(quest, retrieved_docs))\n",
    "    \n",
    "print(\"--- Done ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{P@3: 0.5066666666666667, nDCG@3: 0.5767568351326005, RR: 0.6058333333333334}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = [Precision@3, MRR, nDCG@3]\n",
    "\n",
    "result = ir_measures.calc_aggregate(metrics, ir_qrels, ir_runs)\n",
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stock_chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
